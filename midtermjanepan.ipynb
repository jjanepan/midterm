{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L_IkhHTVWwqh",
        "outputId": "bbba72b7-3dc9-4a6f-f8d6-2cb4fb242735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train length is  1485341\n",
            "X_submission length is  212192\n",
            "Now X_train length is  100000\n",
            "Now X_submission length is  212192\n",
            "              Id   ProductId          UserId        Time  \\\n",
            "578158   1490873  B005LAIIQC  A37L1OGFD7SB2I  1355875200   \n",
            "249432   1149956  B0015LPS1Y   ARC10GZN44C34  1403568000   \n",
            "1364280    24661  0780020693  A2NUD9S80DZRQG  1209945600   \n",
            "441970    696595  B000063V8U  A2M2MUKWB2TRVL  1399766400   \n",
            "906438    305805  6302287375  A338L6RMPYT3ZR  1333411200   \n",
            "\n",
            "                                       Summary  \\\n",
            "578158                         nothing special   \n",
            "249432                           love stargate   \n",
            "1364280                 on the nostalgia wings   \n",
            "441970   Part of one of the finest series ever   \n",
            "906438             Love that but disappointed.   \n",
            "\n",
            "                                                      Text  Helpfulness  \n",
            "578158   some great moments, nice camera work and great...          0.0  \n",
            "249432   We had been looking for this series for quite ...          0.0  \n",
            "1364280  It's a sincere documentary of the era it depic...          0.0  \n",
            "441970   It is simply brilliant.... well written, power...          0.0  \n",
            "906438   I always loved that movie since I was a little...          1.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movie: 6991\n",
            "film: 3494\n",
            "one: 3207\n",
            "like: 2980\n",
            "would: 2138\n",
            "even: 1972\n",
            "bad: 1776\n",
            "time: 1657\n",
            "good: 1617\n",
            "could: 1501\n",
            "really: 1482\n",
            "get: 1481\n",
            "see: 1375\n",
            "story: 1259\n",
            "movies: 1256\n",
            "people: 1255\n",
            "first: 1220\n",
            "much: 1206\n",
            "dvd: 1149\n",
            "make: 1107\n",
            "made: 1015\n",
            "watch: 958\n",
            "way: 955\n",
            "never: 899\n",
            "acting: 885\n",
            "better: 885\n",
            "know: 881\n",
            "well: 860\n",
            "think: 851\n",
            "quot: 846\n",
            "ever: 830\n",
            "plot: 808\n",
            "two: 801\n",
            "characters: 796\n",
            "nothing: 792\n",
            "great: 780\n",
            "character: 770\n",
            "money: 755\n",
            "seen: 743\n",
            "watching: 742\n",
            "show: 738\n",
            "say: 723\n",
            "many: 703\n",
            "back: 692\n",
            "something: 680\n",
            "little: 679\n",
            "want: 678\n",
            "also: 677\n",
            "films: 672\n",
            "original: 654\n",
            "movie: 6869\n",
            "film: 4945\n",
            "one: 3709\n",
            "like: 3206\n",
            "good: 2399\n",
            "would: 2337\n",
            "really: 2048\n",
            "even: 1932\n",
            "story: 1821\n",
            "much: 1798\n",
            "get: 1623\n",
            "time: 1564\n",
            "see: 1514\n",
            "could: 1511\n",
            "first: 1403\n",
            "bad: 1363\n",
            "well: 1215\n",
            "make: 1204\n",
            "better: 1203\n",
            "two: 1184\n",
            "movies: 1149\n",
            "way: 1143\n",
            "character: 1130\n",
            "great: 1129\n",
            "characters: 1093\n",
            "people: 1078\n",
            "made: 1069\n",
            "think: 1040\n",
            "also: 1017\n",
            "quot: 997\n",
            "plot: 994\n",
            "dvd: 990\n",
            "little: 964\n",
            "know: 946\n",
            "many: 935\n",
            "watch: 920\n",
            "never: 895\n",
            "scenes: 888\n",
            "acting: 877\n",
            "end: 839\n",
            "films: 814\n",
            "going: 797\n",
            "best: 791\n",
            "say: 783\n",
            "something: 751\n",
            "go: 750\n",
            "scene: 737\n",
            "love: 736\n",
            "show: 725\n",
            "nothing: 715\n",
            "movie: 11890\n",
            "film: 9847\n",
            "one: 6849\n",
            "like: 5738\n",
            "good: 5456\n",
            "would: 4272\n",
            "story: 3799\n",
            "much: 3766\n",
            "really: 3549\n",
            "time: 3163\n",
            "even: 2936\n",
            "first: 2930\n",
            "get: 2812\n",
            "see: 2810\n",
            "well: 2802\n",
            "great: 2779\n",
            "could: 2594\n",
            "also: 2515\n",
            "better: 2263\n",
            "little: 2243\n",
            "way: 2203\n",
            "dvd: 2196\n",
            "two: 2172\n",
            "character: 2138\n",
            "still: 2124\n",
            "make: 2077\n",
            "think: 2035\n",
            "many: 1994\n",
            "people: 1969\n",
            "bad: 1960\n",
            "characters: 1945\n",
            "quot: 1927\n",
            "movies: 1907\n",
            "watch: 1893\n",
            "best: 1770\n",
            "know: 1753\n",
            "scenes: 1738\n",
            "made: 1726\n",
            "never: 1718\n",
            "love: 1717\n",
            "though: 1687\n",
            "films: 1680\n",
            "plot: 1643\n",
            "new: 1628\n",
            "end: 1611\n",
            "show: 1580\n",
            "pretty: 1523\n",
            "back: 1480\n",
            "life: 1435\n",
            "series: 1435\n",
            "movie: 20275\n",
            "film: 18594\n",
            "one: 13725\n",
            "good: 10589\n",
            "like: 10002\n",
            "great: 7341\n",
            "story: 7022\n",
            "would: 6786\n",
            "well: 6509\n",
            "time: 6190\n",
            "really: 6110\n",
            "much: 5648\n",
            "first: 5622\n",
            "also: 5578\n",
            "see: 5471\n",
            "get: 5224\n",
            "even: 5014\n",
            "dvd: 4822\n",
            "two: 4384\n",
            "way: 4090\n",
            "still: 4060\n",
            "little: 3956\n",
            "quot: 3896\n",
            "love: 3877\n",
            "movies: 3866\n",
            "people: 3763\n",
            "best: 3743\n",
            "could: 3727\n",
            "many: 3726\n",
            "show: 3684\n",
            "watch: 3655\n",
            "character: 3653\n",
            "series: 3564\n",
            "films: 3533\n",
            "think: 3484\n",
            "characters: 3413\n",
            "life: 3382\n",
            "make: 3351\n",
            "new: 3344\n",
            "better: 3242\n",
            "made: 3189\n",
            "know: 3115\n",
            "back: 3077\n",
            "man: 3045\n",
            "never: 3029\n",
            "scenes: 2968\n",
            "though: 2893\n",
            "end: 2856\n",
            "action: 2844\n",
            "lot: 2611\n",
            "movie: 38154\n",
            "one: 27350\n",
            "film: 26714\n",
            "great: 19726\n",
            "like: 16181\n",
            "good: 14248\n",
            "love: 12461\n",
            "time: 12324\n",
            "story: 11962\n",
            "dvd: 11497\n",
            "well: 10695\n",
            "see: 10688\n",
            "would: 10436\n",
            "really: 10139\n",
            "first: 10082\n",
            "also: 9845\n",
            "best: 9638\n",
            "get: 9479\n",
            "show: 9347\n",
            "series: 8998\n",
            "even: 8797\n",
            "movies: 8622\n",
            "watch: 8578\n",
            "much: 8267\n",
            "many: 7277\n",
            "quot: 7117\n",
            "two: 6999\n",
            "people: 6825\n",
            "life: 6774\n",
            "season: 6693\n",
            "way: 6613\n",
            "never: 6231\n",
            "seen: 6084\n",
            "could: 6009\n",
            "still: 5997\n",
            "characters: 5939\n",
            "new: 5766\n",
            "made: 5746\n",
            "think: 5695\n",
            "make: 5678\n",
            "films: 5646\n",
            "know: 5485\n",
            "ever: 5483\n",
            "years: 5439\n",
            "back: 5433\n",
            "man: 5213\n",
            "little: 5099\n",
            "better: 5059\n",
            "character: 5030\n",
            "set: 5030\n",
            "movie: 31285\n",
            "one: 24143\n",
            "film: 23220\n",
            "great: 18946\n",
            "like: 13201\n",
            "good: 12631\n",
            "love: 12461\n",
            "time: 10760\n",
            "story: 10703\n",
            "dvd: 10507\n",
            "well: 9835\n",
            "best: 9638\n",
            "see: 9313\n",
            "also: 9168\n",
            "series: 8998\n",
            "first: 8862\n",
            "really: 8657\n",
            "show: 8622\n",
            "would: 8298\n",
            "get: 7998\n",
            "Loading GloVe embeddings...\n",
            "File not found: glove.6B/glove.6B.300d.txt. Please ensure the file path is correct.\n",
            "Preprocessing halted due to missing GloVe embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 75000 entries, 578158 to 1119578\n",
            "Data columns (total 7 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   Id           75000 non-null  int64  \n",
            " 1   ProductId    75000 non-null  object \n",
            " 2   UserId       75000 non-null  object \n",
            " 3   Time         75000 non-null  int64  \n",
            " 4   Summary      74998 non-null  object \n",
            " 5   Text         74997 non-null  object \n",
            " 6   Helpfulness  75000 non-null  float64\n",
            "dtypes: float64(1), int64(2), object(4)\n",
            "memory usage: 4.6+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set =  0.58052\n",
            "Classification Report:                precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.59      0.27      0.37      1558\n",
            "         2.0       0.36      0.08      0.13      1409\n",
            "         3.0       0.50      0.16      0.24      3012\n",
            "         4.0       0.43      0.14      0.21      5629\n",
            "         5.0       0.60      0.95      0.73     13392\n",
            "\n",
            "    accuracy                           0.58     25000\n",
            "   macro avg       0.50      0.32      0.34     25000\n",
            "weighted avg       0.54      0.58      0.50     25000\n",
            "\n",
            "Confusion Matrix:  [[  425    85    73    48   927]\n",
            " [  139   109   138   102   921]\n",
            " [   54    78   486   336  2058]\n",
            " [   44    16   185   776  4608]\n",
            " [   54    16    82   523 12717]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f34878df-1c5e-4bcb-ad7b-a46c8be17396\", \"submission.csv\", 2407528)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import scipy.sparse as sp\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Ensure all necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load datasets\n",
        "trainingSet = pd.read_csv(\"train.csv\")\n",
        "testingSet = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Feature engineering function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens if word not in stop_words])\n",
        "\n",
        "# Add and preprocess features\n",
        "def add_features_to(df):\n",
        "    df['Helpfulness'] = df['HelpfulnessNumerator'] / (df['HelpfulnessDenominator'] + 0.01)\n",
        "    df['ProcessedText'] = df['Text'].apply(preprocess_text)\n",
        "    return df\n",
        "if not exists('X_train.csv'):\n",
        "    trainingSet = add_features_to(trainingSet)\n",
        "    testingSet = add_features_to(testingSet)\n",
        "    trainingSet.to_csv('X_train.csv', index=False)\n",
        "    testingSet.to_csv('X_submission.csv', index=False)\n",
        "else:\n",
        "    trainingSet = pd.read_csv('X_train.csv')\n",
        "    testingSet = pd.read_csv('X_submission.csv')\n",
        "\n",
        "\n",
        "# Load the feature extracted files\n",
        "if exists('X_train.csv'):\n",
        "    X_train = pd.read_csv(\"X_train.csv\")\n",
        "if exists('X_submission.csv'):\n",
        "    X_submission = pd.read_csv(\"X_submission.csv\")\n",
        "\n",
        "else:\n",
        "    # Process the DataFrame\n",
        "    train = add_features_to(trainingSet)\n",
        "    # Merge\n",
        "    X_submission = pd.merge(train, testingSet, left_on='Id', right_on='Id')\n",
        "    X_submission = X_submission.drop(columns=['Score_x'])\n",
        "    X_submission = X_submission.rename(columns={'Score_y': 'Score'})\n",
        "    X_train =  train[train['Score'].notnull()]\n",
        "\n",
        "    X_submission.to_csv(\"X_submission.csv\", index=False)\n",
        "    X_train.to_csv(\"X_train.csv\", index=False)\n",
        "\n",
        "print(\"X_train length is \", len(X_train))\n",
        "print(\"X_submission length is \", len(X_submission))\n",
        "\n",
        "n = 100000\n",
        "X_train = X_train.sample(n, random_state=42)\n",
        "print(\"Now X_train length is \", len(X_train))\n",
        "print(\"Now X_submission length is \", len(X_submission))\n",
        "# Splitting the set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_train.drop(columns=['Score']),\n",
        "    X_train['Score'],\n",
        "    test_size=1/4.0,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "\n",
        "print(X_train.head())\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_most_common_words(text_series, n=50):\n",
        "    all_text = ' '.join(text_series.fillna('').astype(str))\n",
        "\n",
        "\n",
        "    words = word_tokenize(all_text.lower())\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "\n",
        "    word_freq = Counter(words)\n",
        "    return word_freq.most_common(n)\n",
        "\n",
        "df_combined = pd.concat([X_train, Y_train], axis=1)\n",
        "\n",
        "for rating in sorted(df_combined['Score'].unique()):\n",
        "    class_words = get_most_common_words(df_combined[df_combined['Score'] == rating]['Text'])\n",
        "    for word, count in class_words:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "all_words = set()\n",
        "class_word_freq = {}\n",
        "\n",
        "for rating in sorted(df_combined['Score'].unique()):\n",
        "    class_words = dict(get_most_common_words(df_combined[df_combined['Score'] == rating]['Text']))\n",
        "    class_word_freq[rating] = class_words\n",
        "    all_words.update(class_words.keys())\n",
        "\n",
        "word_diff = {}\n",
        "for word in all_words:\n",
        "    freqs = [class_word_freq[rating].get(word, 0) for rating in sorted(df_combined['Score'].unique())]\n",
        "    word_diff[word] = max(freqs) - min(freqs)\n",
        "\n",
        "for word, diff in sorted(word_diff.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
        "    print(f\"{word}: {diff}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initializing\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def load_glove_embeddings(file_path):\n",
        "    try:\n",
        "        print(\"Loading GloVe embeddings...\")\n",
        "        embeddings = {}\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                embeddings[word] = vector\n",
        "        print(\"GloVe Embeddings Loaded\")\n",
        "        return embeddings\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}. Please ensure the file path is correct.\")\n",
        "        return None\n",
        "\n",
        "# Set GloVe file path and load embeddings\n",
        "glove_path = 'glove.6B/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "def preprocess_words(text):\n",
        "    \"\"\"preprocssing test\"\"\"\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords_set]\n",
        "    return words\n",
        "\n",
        "def get_sentence_vector(sentence, model):\n",
        "    \"\"\"Get the average vector of setnence\"\"\"\n",
        "    words = preprocess_words(sentence)\n",
        "    vectors = [model.get(word) for word in words if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
        "\n",
        "def get_sentiment_score(sentence):\n",
        "    if isinstance(sentence, float):\n",
        "        sentence = str(sentence)\n",
        "    preprocessed_sentence = ' '.join(preprocess_words(sentence))\n",
        "    return analyzer.polarity_scores(preprocessed_sentence)['compound']\n",
        "\n",
        "def preprocess_dataframe(df, text_column, summary_column, model):\n",
        "    print(f\"Processing text vectors for {text_column}...\")\n",
        "    df['text_vector'] = df[text_column].apply(lambda x: get_sentence_vector(x, model))\n",
        "\n",
        "    print(\"sentiment scores:\")\n",
        "    df['text_sentiment'] = df[text_column].apply(get_sentiment_score)\n",
        "    df['summary_sentiment'] = df[summary_column].apply(get_sentiment_score)\n",
        "\n",
        "    # Length\n",
        "    print(\"text lengths:\")\n",
        "    df['text_length'] = df[text_column].fillna('').apply(len)\n",
        "    df['summary_length'] = df[summary_column].fillna('').apply(len)\n",
        "\n",
        "    df = df.drop(columns=[text_column, summary_column])\n",
        "    print(\"Dataframe preprocessing complete.\")\n",
        "    return df\n",
        "\n",
        "if glove_embeddings:\n",
        "    X_train = preprocess_dataframe(X_train, 'Text', 'Summary', glove_embeddings)\n",
        "    X_test = preprocess_dataframe(X_test, 'Text', 'Summary', glove_embeddings)\n",
        "    X_submission = preprocess_dataframe(X_submission, 'Text', 'Summary', glove_embeddings)\n",
        "\n",
        "#dropping columns\n",
        "    columns_to_drop = ['ProductId', 'UserId']\n",
        "    X_train.drop(columns=columns_to_drop, inplace=True)\n",
        "    X_test.drop(columns=columns_to_drop, inplace=True)\n",
        "    X_submission.drop(columns=columns_to_drop, inplace=True)\n",
        "else:\n",
        "    print(\"Preprocessing halted due to missing GloVe embeddings.\")\n",
        "print(X_train.info())\n",
        "\n",
        "\n",
        "X_train.to_csv(\"X_train_preprocessed.csv\", index=False)\n",
        "Y_train.to_csv(\"Y_train_preprocessed.csv\", index=False)\n",
        "X_test.to_csv(\"X_text_preprocessed.csv\", index=False)\n",
        "Y_test.to_csv(\"Y_test_preprocessed.csv\", index=False)\n",
        "X_submission.to_csv(\"X_submission_preprocessed.csv\", index=False)\n",
        "X_train = pd.read_csv(\"X_train_preprocessed.csv\")\n",
        "Y_train = pd.read_csv(\"Y_train_preprocessed.csv\")\n",
        "X_test = pd.read_csv(\"X_text_preprocessed.csv\")\n",
        "Y_test = pd.read_csv(\"Y_test_preprocessed.csv\")\n",
        "X_submission = pd.read_csv(\"X_submission_preprocessed.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Vectorization with TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(trainingSet['ProcessedText'])\n",
        "X_test_tfidf = tfidf.transform(testingSet['ProcessedText'])\n",
        "\n",
        "summary_tfidf_train = tfidf.fit_transform(X_train['Summary'].fillna(''))\n",
        "summary_tfidf_test = tfidf.transform(X_test['Summary'].fillna(''))\n",
        "summary_tfidf_submission = tfidf.transform(X_submission['Summary'].fillna(''))\n",
        "\n",
        "summary_count_train = count_vectorizer.fit_transform(X_train['Summary'].fillna(''))\n",
        "summary_count_test = count_vectorizer.transform(X_test['Summary'].fillna(''))\n",
        "summary_count_submission = count_vectorizer.transform(X_submission['Summary'].fillna(''))\n",
        "\n",
        "numeric_features = ['text_sentiment', 'text_length', 'summary_length']\n",
        "\n",
        "X_train_text_vector = np.vstack(X_train['text_vector'].values) if 'text_vector' in X_train.columns else np.zeros((X_train.shape[0], 300))\n",
        "X_test_text_vector = np.vstack(X_test['text_vector'].values) if 'text_vector' in X_test.columns else np.zeros((X_test.shape[0], 300))\n",
        "X_submission_text_vector = np.vstack(X_submission['text_vector'].values) if 'text_vector' in X_submission.columns else np.zeros((X_submission.shape[0], 300))\n",
        "\n",
        "# Combining features\n",
        "X_train_combined = sp.hstack([\n",
        "    sp.csr_matrix(X_train_text_vector),\n",
        "    summary_tfidf_train,\n",
        "])\n",
        "\n",
        "X_test_combined = sp.hstack([\n",
        "    sp.csr_matrix(X_test_text_vector),\n",
        "    summary_tfidf_test,\n",
        "])\n",
        "\n",
        "X_submission_combined = sp.hstack([\n",
        "    sp.csr_matrix(X_submission_text_vector),\n",
        "    summary_tfidf_submission,\n",
        "])\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y_train_encoded = le.fit_transform(Y_train)\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    booster='gbtree',\n",
        "    num_class=5,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    min_child_weight=3,\n",
        "    n_estimators=1000,\n",
        "    gamma=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Learn the model\n",
        "model.fit(X_train_combined, Y_train_encoded)\n",
        "\n",
        "Y_test_predictions = model.predict(X_test_combined)\n",
        "y_pred = le.inverse_transform(Y_test_predictions)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = model.predict(X_val_tfidf)\n",
        "predictions = encoder.inverse_transform(predictions)\n",
        "\n",
        "print(\"Accuracy on testing set:\", accuracy_score(y_val, predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, predictions))\n",
        "\n",
        "Y_submission_predictions = model.predict(X_submission_combined)\n",
        "X_submission['Score'] = le.inverse_transform(Y_submission_predictions)\n",
        "submission = X_submission[['Id', 'Score']]\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ]
    }
  ]
}